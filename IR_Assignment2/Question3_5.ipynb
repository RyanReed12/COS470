{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Ryan Reed - COS 470 - Assignment 2 - Question 3"
      ],
      "metadata": {
        "id": "Drtx5u57jflS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 193,
      "metadata": {
        "id": "oW4ryPNfi0b2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d61c5a9-ed24-46f7-c188-f0cbddad9d5f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "# importing\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from post_parser_record import PostParserRecord\n",
        "from collections import defaultdict\n",
        "import re\n",
        "\n",
        "# creating the post_reader object to read in questions & answers\n",
        "post_reader = PostParserRecord(\"Posts_Coffee.xml\")\n",
        "# initializing a set that contains stop words, for easier removal operations\n",
        "stop_words = set(stopwords.words('english'))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# utility function to clean input text.\n",
        "# full functions are : conversion to lowercase, removing stopwords, removing punctuation.\n",
        "# I implemented this seperately for purposes of code readability, as well as simplification.\n",
        "def text_processor (input_text):\n",
        "  # remove punctuation & convert text to lowercase\n",
        "  input_text = re.sub(r\"[(,,.;@/>#'â€™//=\\\"-:?*\\[\\]<!&$)]+\\ *\", \" \", input_text.lower())\n",
        "  # text tokenized into a list\n",
        "  list_of_words = word_tokenize(input_text)\n",
        "  # list comprehension to filter out any stopwords\n",
        "  list_of_words_res = [x for x in list_of_words if (x not in stop_words) and (x != 'p')]\n",
        "  # slicing off the xml headers, and returning it\n",
        "  # Reasoning for returning as a set: duplicate removal.\n",
        "  return set(list_of_words_res)"
      ],
      "metadata": {
        "id": "BcCjz3ONpML3"
      },
      "execution_count": 194,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# inverted index function that creates our hashtable object for a collection of documents\n",
        "# the python implementation of a hashtable is a dictionary, and our collection is the set\n",
        "# of questions and answers within the snapshot of the coffee stack exchange.\n",
        "def InvertedIndex(reader_object):\n",
        "  #initializing empty dictionary\n",
        "  II_dict = {}\n",
        "  count = 0\n",
        "\n",
        "  # the function technically has two collections that it will need to go over.\n",
        "  # the first are the posts that are questions, the second are the posts that are answers.\n",
        "  \n",
        "  # for question posts\n",
        "  for post in reader_object.map_questions:\n",
        "    \n",
        "    # text_processor function parses the post title and body into tokens.\n",
        "    # text_processor also performs data processing, documented in the function code block.\n",
        "    # returned as a set, no need to worry about duplicates.\n",
        "    set_of_tokens = text_processor(reader_object.map_questions[post].title + \" \" + reader_object.map_questions[post].body)\n",
        "    for token in set_of_tokens:\n",
        "      if (token not in II_dict):\n",
        "        II_dict[token] = set()\n",
        "      II_dict[token].add(reader_object.map_questions[post].post_id)\n",
        "      \n",
        "  \n",
        "  # same as the process for questions, but for answers.\n",
        "  for post in reader_object.map_just_answers:\n",
        "    # same as for questions\n",
        "    set_of_tokens = text_processor(reader_object.map_just_answers[post].body)\n",
        "    for token in set_of_tokens:\n",
        "      if (token not in II_dict):\n",
        "        II_dict[token] = set()\n",
        "      II_dict[token].add(reader_object.map_just_answers[post].post_id)\n",
        "  \n",
        "  # sorting the index sets in the dictionary.\n",
        "  # this algorithm is merging the indexes of tokens in questions and answers.\n",
        "  # so we must sort the sets of indexes, as we merge the tokens from both types of posts.\n",
        "  for key in II_dict:\n",
        "    II_dict[key] = sorted(II_dict[key])\n",
        "  return II_dict"
      ],
      "metadata": {
        "id": "XFafsxnjllEm"
      },
      "execution_count": 195,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# runs the function with the post_reader\n",
        "inverted_index = InvertedIndex(post_reader)"
      ],
      "metadata": {
        "id": "AdKC2hYPI8jN"
      },
      "execution_count": 196,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# this is for error checking, ensuring that the correct postings are found\n",
        "# without any issues.\n",
        "\n",
        "# print(inverted_index['espresso'])"
      ],
      "metadata": {
        "id": "OuP7rYIYdpnM"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# this function handles both querying, and computing similiarity score\n",
        "# implementing operators would be easy, but the assignment doesn't\n",
        "# request that. so it'll be explicit AND for more than one term.\n",
        "def query(input_terms):\n",
        "  # creates a temporary dictionary\n",
        "  temp_dict = dict()\n",
        "  # single term query\n",
        "  # the loop below returns a dataframe \n",
        "  if len(input_terms) == 1:\n",
        "    term = input_terms[0]\n",
        "    if term in inverted_index:\n",
        "      temp_dict = {'Query': [term], 'PostIDs': [inverted_index[term][:10]], 'Similarity Score' : ['N/A']}\n",
        "      return pd.DataFrame(temp_dict)\n",
        "  # two-term query, explicit AND for this assignment\n",
        "  if len(input_terms) == 2:\n",
        "    # obtains terms\n",
        "    term1, term2 = input_terms\n",
        "    set_indices = set()\n",
        "    # query_string is simply for conversion later on into a dataframe.\n",
        "    query_string = term1, \" AND \", term2\n",
        "    similarity_score = 0.0\n",
        "    if term1 in inverted_index:\n",
        "      if term2 in inverted_index:\n",
        "        # for computing similarity score, I'm assuming based on the assignment that\n",
        "        # document count means the total number of documents each appears in independently.\n",
        "        for index1 in inverted_index[term1]:\n",
        "          if index1 in inverted_index[term2]:\n",
        "            set_indices.add(index1)\n",
        "        \n",
        "        set_indices = sorted(set_indices)\n",
        "\n",
        "        len_term1 = len(inverted_index[term1])\n",
        "        len_term2 = len(inverted_index[term2])\n",
        "\n",
        "        # calculation of similarity scores\n",
        "        if (len_term1 > len_term2):\n",
        "          similarity_score = (1- (len(inverted_index[term1]) - len(inverted_index[term2])) / (len(inverted_index[term1])))\n",
        "        else:\n",
        "          similarity_score = (1 - (len(inverted_index[term2]) - len(inverted_index[term1])) / (len(inverted_index[term2])))\n",
        "\n",
        "        temp_dict = {'Query': (''.join(query_string)), 'PostIDs': [set_indices[:10]],\n",
        "                  'Similarity Score' : [similarity_score]}\n",
        "    # handles the instance of a term not appearing in the collection.\n",
        "    # in our three queries, persian AND coffee meets this. \n",
        "    elif (term1 not in inverted_index) or (term2 not in inverted_index):\n",
        "      temp_dict = {'Query': (''.join(query_string)), 'PostIDs': ['N/A'], 'Similarity Score' : [similarity_score]}\n",
        "    return pd.DataFrame(temp_dict)\n",
        "  # number of arguments must be 1 or 2, anything more will be returned with an error statement.\n",
        "  else:\n",
        "    print(\"Error: Incorrect number of terms for Query.\")\n",
        "    return 0"
      ],
      "metadata": {
        "id": "Jg4hGJBYJltL"
      },
      "execution_count": 198,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# queries\n",
        "# handles each query seperately, and merges the returned dataframes into one singular one.\n",
        "\n",
        "q1 = query([\"espresso\"])\n",
        "q2 = query([\"turkish\", \"coffee\"])\n",
        "q3 = query([\"persian\", \"coffee\"])\n",
        "queries_result = pd.concat([q1, q2, q3])\n",
        "queries_result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "id": "Jjzro6iGH7cb",
        "outputId": "f2b1f2c6-27c4-4408-c3df-df1f5876181b"
      },
      "execution_count": 210,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                Query                                          PostIDs  \\\n",
              "0            espresso             [2, 5, 7, 9, 10, 17, 22, 26, 27, 30]   \n",
              "0  turkish AND coffee  [42, 45, 81, 106, 165, 209, 216, 349, 365, 419]   \n",
              "0  persian AND coffee                                              N/A   \n",
              "\n",
              "  Similarity Score  \n",
              "0              N/A  \n",
              "0         0.036829  \n",
              "0              0.0  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-d5cf6cc7-f89c-4918-a381-d39bed0eaf5f\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Query</th>\n",
              "      <th>PostIDs</th>\n",
              "      <th>Similarity Score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>espresso</td>\n",
              "      <td>[2, 5, 7, 9, 10, 17, 22, 26, 27, 30]</td>\n",
              "      <td>N/A</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>turkish AND coffee</td>\n",
              "      <td>[42, 45, 81, 106, 165, 209, 216, 349, 365, 419]</td>\n",
              "      <td>0.036829</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>persian AND coffee</td>\n",
              "      <td>N/A</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d5cf6cc7-f89c-4918-a381-d39bed0eaf5f')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-d5cf6cc7-f89c-4918-a381-d39bed0eaf5f button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-d5cf6cc7-f89c-4918-a381-d39bed0eaf5f');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 210
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# writing indexes to tsv\n",
        "import csv\n",
        "\n",
        "with open('indexes.tsv', 'w') as csvfile:\n",
        "  writer = csv.writer(csvfile, delimiter='\\t')\n",
        "\n",
        "  indexes_dict = dict()\n",
        "  for x in inverted_index:\n",
        "    indexes_dict[x] = inverted_index[x]\n",
        "  writer.writerows(indexes_dict.items())"
      ],
      "metadata": {
        "id": "2LztlKFqr6Zc"
      },
      "execution_count": 209,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# calculation of kendall's tau correlation\n",
        "# from compiled lists of P@10 and MRR.\n",
        "import scipy.stats as stats\n",
        "MRR_list = [0.6103, 0.4339 , 0.1003 ,0.0854, 0.8640, 0.2811, 0.4829, 0.0237, 0.4196, 0.3857, 0.2432]\n",
        "P_10_list = [0.3566, 0.1818, 0.0584 ,0.0519, 0.5479, 0.1883, 0.2974, 0.0039, 0.2403 , 0.2390 , 0.1390]\n",
        "\n",
        "tau, pvalue = stats.kendalltau(MRR_list, P_10_list)\n",
        "print(tau)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YagVFFXwS4l4",
        "outputId": "5c9c881a-941f-4bcf-b96c-f2b814680d43"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.8909090909090909\n"
          ]
        }
      ]
    }
  ]
}